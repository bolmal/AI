import asyncio
from crawl4ai import AsyncWebCrawler, CacheMode
from crawl4ai.async_configs import CrawlerRunConfig
from bs4 import BeautifulSoup  # HTML íŒŒì‹±ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
from urllib.parse import urljoin  # ì ˆëŒ€ ê²½ë¡œ ë³€í™˜ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os

from markdown_it.rules_core.normalize import NULL_RE
from openai import OpenAI
from typing import List, Dict
import time
import json
from datetime import datetime
from random import uniform
import requests
import math

from selenium import webdriver
from selenium.webdriver.common.proxy import Proxy
from selenium.webdriver.common.proxy import ProxyType
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
API_URL = "https://dev.bolmal.shop/save"

# ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì„¤ì •
options = Options()
options.page_load_strategy = 'normal'
options.add_experimental_option("detach", True)
driver = webdriver.Chrome(options=options)


class ConcertParser:
    def __init__(self):
        self.client = OpenAI(api_key=OPENAI_API_KEY)
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” íŒŒì‹± ê·œì¹™ê³¼ ì¶œë ¥ í¬ë§·ì— ì§‘ì¤‘
        self.system_message = """You are a Korean concert information parser.
        Your task is to extract structured data from concert information text.
        Always output valid JSON in Korean following this exact format:
        {
            "concert_name": string,        // ê³µì—°ëª…
            "concert_poster:" string | null,  // í¬ìŠ¤í„°      
            "genre": string,              // ì¥ë¥´ (í•˜ë‚˜ë§Œ ì„ íƒ)
            "concert_mood": string,       // ê³µì—° ë¶„ìœ„ê¸° (í•˜ë‚˜ë§Œ ì„ íƒ)
            "concert_style": string,      // ê³µì—° ìŠ¤íƒ€ì¼ (í•˜ë‚˜ë§Œ ì„ íƒ)
            "concert_type": string,       // ê³µì—° ìœ í˜• (í•˜ë‚˜ë§Œ ì„ íƒ)
            "casting": [                   // ìºìŠ¤íŒ…
                {
                    "name": string
                }
            ]                  
            "performance_rounds": [        // ê³µì—° íšŒì°¨ ì •ë³´
                {
                    "round": number,
                    "datetime": "YYYY-MM-DDTHH:MM:SS"
                }
            ],
            "venue": string | null,        // ê³µì—°ì¥ì†Œ
            "running_time": number | null, // ëŸ¬ë‹íƒ€ì„(ë¶„)
            "price": {                     // ê°€ê²© ì •ë³´
                "type": number             // ì¢Œì„ ìœ í˜•ë³„ ê°€ê²©
            },
            "age_limit": string | null,    // ê´€ëŒì—°ë ¹
            "booking_limit": string | null, // ì˜ˆë§¤ì œí•œ
            "selling_platform": "INTERPARK",  // íŒë§¤ì²˜
            "ticket_status": boolen,       // í‹°ì¼“ ì˜¤í”ˆ ì—¬ë¶€
            "ticket_open_dates": {       // í‹°ì¼“ ì˜¤í”ˆ ì¼ì •
                    "round": YYYY-MM-DDTHH:MM:SS
                }
            "booking_link": string | null,  // ì˜ˆë§¤ ë§í¬
            "additional_info": "ì´ê²ƒì€ í•˜ë‚˜ì˜ ë¬¸ìì—´ì…ë‹ˆë‹¤."  // ì¶”ê°€ì •ë³´
        }"""

    def parse_single_concert(self, concert_text: str) -> dict:
        """ë‹¨ì¼ ê³µì—° ì •ë³´ íŒŒì‹±"""
        prompt = f"""ë‹¤ìŒ ê³µì—° ì •ë³´ë¥¼ íŒŒì‹±í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.

        ê·œì¹™:
        1. ë‚ ì§œì™€ ì‹œê°„ì€ ëª¨ë‘ YYYY-MM-DDTHH:MM:SS í˜•ì‹ì„ ì‚¬ìš©
        2. ê°€ê²©ì€ ìˆ«ìë¡œ ë³€í™˜ (ì˜ˆ: "90,000ì›" â†’ 90000)
        3. ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° null ì‚¬ìš©
        4. ticket_statusëŠ” True ë˜ëŠ” False ì¤‘ í•˜ë‚˜ë¡œ í‘œì‹œ
        5. ticket_open_datesëŠ” ë‹¤ìŒ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ:
           - keyëŠ” ì˜ˆë§¤ ìœ í˜• ë˜ëŠ” íšŒì°¨ ë²ˆí˜¸
           - valueëŠ” YYYY-MM-DDTHH:MM:SS í˜•ì‹ì˜ ë‚ ì§œ
        6. ë‹¤ìŒ í•„ë“œë“¤ì€ ê°ê° ì •í•´ì§„ ê°’ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤:
            genreëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ:
            ["ë°œë¼ë“œ","ëŒ„ìŠ¤","ë©/í™í•©","ì•„ì´ëŒ","R&B/Soul","ì¸ë””ìŒì•…","ë¡/ë©”íƒˆ","ì„±ì¸ê°€ìš”/íŠ¸ë¡œíŠ¸",
            "í¬í¬/ë¸”ë£¨ìŠ¤","ì¼ë ‰íŠ¸ë¡œë‹ˆì¹´","í´ë˜ì‹","ì¬ì¦ˆ","J-POP","POP","í‚¤ì¦ˆ","CCM","êµ­ì•…"]
       
            concert_moodëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ:
            ["Emotional","Energetic","Dreamy","Grand","Calm","Fun","Intense"]
       
            concert_styleëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ:
            ["Live Band","Acoustic","Orchestra","Solo Performance","Dance Performance","Theatrical Concert"]
       
            concert_typeëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ:
            ["Festival","Concert","Music Show","Fan Meeting","Talk Concert"]
        
        ê³µì—° ì •ë³´:
        {concert_text}
        """

        messages = [
            {"role": "system", "content": self.system_message},
            {"role": "user", "content": prompt}
        ]

        try:
            response = self.client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=messages,
                response_format={"type": "json_object"},
                temperature=0.1  # ì¼ê´€ëœ ì¶œë ¥ì„ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©
            )

            # JSON íŒŒì‹± ê²€ì¦
            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            print(f"Error parsing concert data: {e}")
            return None

    def parse_multiple_concerts(self, crawled_concerts: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ ê³µì—° ì •ë³´ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
        results = []
        for idx, concert_text in enumerate(crawled_concerts):
            try:
                # API ë ˆì´íŠ¸ ë¦¬ë°‹ ê³ ë ¤
                if idx > 0:
                    time.sleep(0.5)

                print(f"Parsing concert {idx + 1}/{len(crawled_concerts)}")
                result = self.parse_single_concert(concert_text)

                if result:
                    results.append(result)
                    print(f"Successfully parsed concert {idx + 1}")
                else:
                    print(f"Failed to parse concert {idx + 1}")

            except Exception as e:
                print(f"Error processing concert {idx + 1}: {e}")
                continue

        return results

"""ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ í¬ë¡¤ë§ í•¨ìˆ˜"""
async def crawl_with_retry(max_retries: int = 1):
    min_num_of_error = 100
    final_crawled_concert = False
    parsed_results = False
    for attempt in range(1,max_retries+1):
        try:
            # ëœë¤ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€
            await asyncio.sleep(uniform(3, 5))

            # í¬ë¡¤ë§ ì‹œë„
            crawrled_data, num_of_error = await crawl_and_parse_concerts(page_num=attempt)
            print("ì—ëŸ¬ìˆ˜ :",num_of_error)
            if parsed_results:
                parsed_results.append(crawrled_data)
            else:
                parsed_results=crawrled_data
            # if num_of_error < min_num_of_error:
            #     min_num_of_error = num_of_error
            #     final_crawled_concert = crawrled_data

            #     if parsed_results:
            #         # GPT íŒŒì‹± ì²˜ë¦¬
            #         parser = ConcertParser()
            #         parsed_results.append(parser.parse_multiple_concerts(final_crawled_concert))
            #         print(parsed_results)
            #     else:
            #         # GPT íŒŒì‹± ì²˜ë¦¬
            #         parser = ConcertParser()
            #         parsed_results=parser.parse_multiple_concerts(final_crawled_concert)
            #         print(parsed_results)

            # else:
            #     print("í¬ë¡¤ë§ ë¬¸ì œ")

        except Exception as e:
            print(f"Attempt {attempt + 1} failed for crawling",e)

        await asyncio.sleep(5)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

    return parsed_results

    # if final_crawled_concert:
    #     # GPT íŒŒì‹± ì²˜ë¦¬
    #     parser = ConcertParser()
    #     parsed_results = parser.parse_multiple_concerts(final_crawled_concert)
    #     print(parsed_results)
    #     return parsed_results
    # else:
    #     print("í¬ë¡¤ë§ ë¬¸ì œ")


async def crawl_and_parse_concerts(page_num: int = 0):
    crawled_concerts = []
    errer_crawled_concerts = 0
    config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS, # ìºì‹œì‚¬ìš© X
        wait_for_images=True, # í¬ë¡¤ëŸ¬ëŠ” HTMLì„ ë§ˆë¬´ë¦¬í•˜ê¸° ì „ì— ì´ë¯¸ì§€ ë¡œë”©ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
        scan_full_page=True, # í¬ë¡¤ëŸ¬ì—ê²Œ ìœ„ì—ì„œ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤ì„ ì‹œë„
        scroll_delay=0.5, # ê° ìŠ¤í¬ë¡¤ ë‹¨ê³„ ì‚¬ì´ì— 0.5ì´ˆ ë™ì•ˆ ì¼ì‹œ ì •ì§€
    )
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Cache-Control': 'no-cache',  # ìºì‹œ ë¬´ì‹œ
        'Cookie': ''  # ê¸°ì¡´ ì¿ í‚¤ í´ë¦¬ì–´
    }
    # async with AsyncWebCrawler(verbose=True) as crawler:
    # ë©”ì¸ í˜ì´ì§€(ì¥ë¥´ : ì½˜ì„œíŠ¸) í¬ë¡¤ë§
    url = "https://tickets.interpark.com/contents/notice?Genre=CONCERT"
    # URL ì ‘ì†
    driver.get(url)
    # ì•”ì‹œì  ëŒ€ê¸°
    driver.implicitly_wait(5)

    processed_count = 0
    scroll_amount = 900  # ì„ì˜ì˜ ìŠ¤í¬ë¡¤ ê±°ë¦¬ (px)

    MAX_ITEMS = 15  # ì›í•˜ëŠ” ìµœëŒ€ ì•„ì´í…œ ìˆ˜ ì œí•œ (ì˜ˆ: 100ê°œê¹Œì§€)
    scroll_count = 0
    index = 0
    items = driver.find_elements(By.CSS_SELECTOR, "a.TicketItem_ticketItem__")
    print("ì•„ì´í…œìˆ˜",len(items))
    concert_info = {}
    while  processed_count < MAX_ITEMS:
        if(scroll_count>0 and scroll_count%2==0):
            index -= 5
        for i in range(5):
            # ë§¤ë²ˆ freshí•˜ê²Œ ì¬ì„ ì–¸!
            items = driver.find_elements(By.CSS_SELECTOR, "a.TicketItem_ticketItem__")
            if i >= len(items):
                print(f"âš ï¸ index {i} ì´ˆê³¼ â€” í•­ëª© ê°œìˆ˜ê°€ ì¤„ì—ˆìŒ, ìŠ¤í‚µ")
                continue
            MAX_ITEMS = max(len(items), MAX_ITEMS)
            print("ì•„ì´í…œ ìˆ˜: !!!!!!!",len(items))
            item = items[index]

            label = item.get_attribute("gtm-label")

            # ì´ë¯¸ì§€ ë§í¬
            try:
                img = item.find_element(By.CSS_SELECTOR, "img").get_attribute("src")
            except:
                img = None

            # í…ìŠ¤íŠ¸ ì •ë³´
            try:
                texts = item.find_elements(By.CSS_SELECTOR, "ul.NoticeItem_contentsWrap__y1tdg li")
                for j, t in enumerate(texts):
                    print(f"text{j+1}:", t.text)
            except:
                pass

            print("Image:", img)

            # ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
            href = item.get_attribute("href")
            if href:
                print("Link (a íƒœê·¸ ì§ì ‘):", href)
            else:
                try:
                    item.click()
                    print("í´ë¦­í›„")

                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "article.DetailSummary_infoBox__5we4P"))
                    )

                    current_url = driver.current_url
                    concert_info["booking_link"] = current_url
                    print(f"[{processed_count}] ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ:", current_url)

                    driver.back()

                    # ë‹¤ì‹œ ëª©ë¡ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "a.TicketItem_ticketItem__"))
                    )

                except Exception as e:
                    print(f"[{i+1}] í´ë¦­ ì‹¤íŒ¨:", e)

            print("-" * 50)
            # í¬ë¡¤ë§ëœ ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜
            concert_text = f"""
            ê³µì—°ëª…: {label}
            ê³µì—° í¬ìŠ¤í„°: {img}
            í‹°ì¼“ìƒíƒœ: {concert_info.get('ticket_status', '')}
            ê³µì—°ì •ë³´: {texts}
            ê³µì—°ìš”ì•½: {concert_info.get('summary', '')}
            ê³µì—°ì„¤ëª…: {concert_info.get('description', '')}
            ì˜ˆë§¤ë§í¬: {concert_info.get('booking_link', 'í‹°ì¼“ ë¯¸ì˜¤í”ˆ')}
            """
            print(concert_text)
            crawled_concerts.append(concert_text)
            index+=1
            processed_count +=1

        driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
        print(f"ğŸ“œ ìŠ¤í¬ë¡¤ {scroll_amount}px ë§Œí¼ ë‚´ë¦¼")
        time.sleep(6)  # ë¡œë”© ëŒ€ê¸°
        scroll_count+=1


    return crawled_concerts, errer_crawled_concerts

    #     # iframe ë‚´ë¶€ URL í¬ë¡¤ë§
    #     result = await crawler.arun(
    #         url=url,
    #         headers=headers,
    #         css_selector="td.subject",  # í•„ìš”í•œ ë°ì´í„° ì„ íƒ
    #         # process_iframes=False,
    #         config=config
    #     )
    #     print("actural URL : ",result.url)
        
    #     # BeautifulSoupìœ¼ë¡œ ë°ì´í„° íŒŒì‹±
    #     iframe_soup = BeautifulSoup(result.html, "html.parser")
    #     rows = iframe_soup.select_one("div.InfiniteList_ticket-list__dfe68")
    #     # ê²°ê³¼ ì²˜ë¦¬
    #     for row in rows:
    #         print("row:",row)
    #         await asyncio.sleep(1)
    #         #  ê° ê³µì—° ì •ë³´ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    #         concert_info = {}

    #         # <td class="subject">ì™€ <a> íƒœê·¸ ì²˜ë¦¬
    #         subject_tag = row.select_one("td.subject a")
    #         # ì¥ë¥´ ê°€ì ¸ì˜¤ê¸°
    #         type_tag = row.select_one("td.type")

    #         # ì´ ë¶€ë¶„ì— ì›í•˜ëŠ” íƒ€ì…ì˜ ì¥ë¥´ë§Œ ê°€ì ¸ ì˜¬ ìˆ˜ ìˆë‹¤
    #         if type_tag and type_tag.text:
    #             concert_info['genre'] = type_tag.text.strip()
    #         else:
    #             continue

    #         if not concert_info.get('genre','') in ['ì½˜ì„œíŠ¸','HOT']:
    #             continue

    #         if subject_tag:
    #             concert_info['title'] = subject_tag.get_text(strip=True)
    #             link = subject_tag.get("href")
    #             absolute_link = urljoin(url, link)
    #             print("ë§í¬:",absolute_link)
    #             concert_info['link'] = absolute_link

    #             # new <img> íƒœê·¸ í™•ì¸ (ìƒˆë¡œ ëœ¬ ê³µì§€)
    #             new_img_tag = row.select_one("td.subject img.ico_new")
    #             concert_info['is_new'] = bool(new_img_tag)

    #             print("new img tag :",new_img_tag)

    #             if absolute_link:
    #                 await asyncio.sleep(3)  # ìƒì„¸ë§í¬ í¬ë¡¤ë§ ì „ ëŒ€ê¸°

    #                 async with AsyncWebCrawler(verbose=True) as crawler_concert:
    #                     # ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
    #                     result_concert = await crawler_concert.arun(url=absolute_link,config=config)

    #                     if not result_concert.success:
    #                         print(f"Crawl failed: {result_concert.error_message}")
    #                         print(f"Status code: {result_concert.status_code}")
    #                         errer_crawled_concerts += 1

    #                 # BeautifulSoup ê°ì²´ ìƒì„±
    #                 soup_concert = BeautifulSoup(result_concert.html, 'html.parser')

    #                 # í¬ìŠ¤í„° ì‚¬ì§„
    #                 poster_div = soup_concert.select_one("div.DetailSummary_imageContainer__OmWus")
    #                 if poster_div:
    #                     poster_tag = poster_div.find("img")  # <img> íƒœê·¸ ì°¾ê¸°
    #                     if poster_tag :
    #                         img_url = poster_tag.get("src")
    #                         # í”„ë¡œí† ì½œì´ ì—†ëŠ” ê²½ìš° http: ì¶”ê°€
    #                         if img_url.startswith("//"):
    #                             img_url = "https:" + img_url
    #                         concert_info['poster'] = img_url
    #                 else:
    #                     errer_crawled_concerts += 1
    #                     if poster_div and poster_div.text:
    #                         concert_info['info'] = poster_div.text.strip()

    #                 # ê³µì—° ìš”ì•½ ì •ë³´
    #                 concert_summary_div = soup_concert.select_one("article.DetailSummary_infoBox__5we4P")
    #                 if concert_summary_div and concert_summary_div.text:
    #                     concert_info['summary'] = concert_summary_div.text.strip()
    #                 else:
    #                     print("concert_summary_div ì—†ìŒ ë˜ëŠ” ë‚´ìš© ì—†ìŒ:", concert_summary_div)
    #                     concert_info['summary'] = None
                    
    #                 # ê³µì—° ë””í…Œì¼ ì •ë³´
    #                 concert_detail_div = soup_concert.select_one(".DetailInfo_contents__grsx5.DetailInfo_isOld__4UynI")
    #                 if concert_detail_div and concert_detail_div.text:
    #                     concert_info['description'] = concert_detail_div.text.strip()
    #                 else:
    #                     print("concert_detail_div ì—†ìŒ ë˜ëŠ” ë‚´ìš© ì—†ìŒ:", concert_detail_div)
    #                     concert_info['description'] = None
                        
    #                 # <a class="btn_book"> íƒœê·¸ ì²˜ë¦¬ ì˜ˆì•½ ë²„íŠ¼
    #                 book_button_tag = soup_concert.select_one("button.DetailBooking_bookingBtn__uvSid")
    #                 if book_button_tag:
    #                     concert_info['booking_link'] = absolute_link
    #                     concert_info['ticket_status'] = "True"
    #                 else :
    #                     concert_info['ticket_status'] = "False"

    #                 # í¬ë¡¤ë§ëœ ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜
    #                 concert_text = f"""
    #                 ê³µì—°ëª…: {concert_info.get('title', '')}
    #                 ê³µì—° í¬ìŠ¤í„°: {concert_info.get('poster', '')}
    #                 ì¥ë¥´: {concert_info.get('genre', '')}
    #                 í‹°ì¼“ìƒíƒœ: {concert_info.get('ticket_status', '')}
    #                 ê³µì—°ì •ë³´: {concert_info.get('info', '')}
    #                 ê³µì—°ìš”ì•½: {concert_info.get('summary', '')}
    #                 ê³µì—°ì„¤ëª…: {concert_info.get('description', '')}
    #                 ì˜ˆë§¤ë§í¬: {concert_info.get('booking_link', 'í‹°ì¼“ ë¯¸ì˜¤í”ˆ')}
    #                 """
    #                 print(concert_text)
    #                 crawled_concerts.append(concert_text)

    # return crawled_concerts, errer_crawled_concerts

# ì‹¤í–‰ ì½”ë“œ
async def main():
    # í¬ë¡¤ë§ ì½”ë“œ ì‹¤í–‰
    results = await crawl_with_retry()

    # ê²°ê³¼ ì œì¶œ
    current_dir = os.getcwd()
    target_dir = os.path.join(current_dir, 'crawl_new_concerts')
    os.makedirs(target_dir, exist_ok=True)
    # í˜„ì¬ ë‚ ì§œë¥¼ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì €ì¥
    current_date = datetime.now().strftime('%Y-%m-%d')  # 'YYYY-MM-DD' í˜•ì‹
    file_path = os.path.join(target_dir, f'{current_date}.json')

    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # HTTP í—¤ë” ì„¤ì • (JSON í˜•ì‹)
    headers = {"Content-Type": "application/json"}

    # # JSON íŒŒì¼ ì—´ê¸° ë° ë°ì´í„° ë¡œë“œ
    # with open(f'crawl_new_concerts/{file_path}', 'r', encoding='utf-8') as f:
    #     data_list = json.load(f)  # ì´ì œ data_listëŠ” Python ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤

    # response = requests.post(API_URL, headers=headers, json=data_list)  # `json=data` ì‚¬ìš©

    # ì‘ë‹µ í™•ì¸
    # print("ì‘ë‹µ ì½”ë“œ:", response.status_code)

if __name__ == "__main__":
    asyncio.run(main())

