import asyncio
from crawl4ai import AsyncWebCrawler, CacheMode
from crawl4ai.async_configs import CrawlerRunConfig
from bs4 import BeautifulSoup  # HTML íŒŒì‹±ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
from urllib.parse import urljoin  # ì ˆëŒ€ ê²½ë¡œ ë³€í™˜ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import html
from openai import OpenAI
from typing import List, Dict
import time
import json
from datetime import datetime
from random import uniform

from selenium import webdriver
from selenium.webdriver.common.proxy import Proxy
from selenium.webdriver.common.proxy import ProxyType
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
API_URL = "https://dev.bolmal.shop/save"

# ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì„¤ì •
options = Options()
options.page_load_strategy = 'normal'
options.add_experimental_option("detach", True)
driver = webdriver.Chrome(options=options)


class ConcertParser:
    def __init__(self):
        self.client = OpenAI(api_key=OPENAI_API_KEY)
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” íŒŒì‹± ê·œì¹™ê³¼ ì¶œë ¥ í¬ë§·ì— ì§‘ì¤‘
        self.system_message = """You are a Korean concert information parser.
        Your task is to extract structured data from concert information text.
        Always output valid JSON in Korean following this exact format:
        {
            "concert_name": string,        // ê³µì—°ëª…
            "concert_poster:" string | null,  // í¬ìŠ¤í„°      
            "genre": string,              // ì¥ë¥´ (í•˜ë‚˜ë§Œ ì„ íƒ)
            "concert_mood": string,       // ê³µì—° ë¶„ìœ„ê¸° (í•˜ë‚˜ë§Œ ì„ íƒ)
            "concert_style": string,      // ê³µì—° ìŠ¤íƒ€ì¼ (í•˜ë‚˜ë§Œ ì„ íƒ)
            "concert_type": string,       // ê³µì—° ìœ í˜• (í•˜ë‚˜ë§Œ ì„ íƒ)
            "casting": [                   // ìºìŠ¤íŒ…
                {
                    "name": string
                }
            ]                  
            "performance_rounds": [        // ê³µì—° íšŒì°¨ ì •ë³´
                {
                    "round": number,
                    "datetime": "YYYY-MM-DDTHH:MM:SS"
                }
            ],
            "venue": string | null,        // ê³µì—°ì¥ì†Œ
            "running_time": number | null, // ëŸ¬ë‹íƒ€ì„(ë¶„)
            "price": {                     // ê°€ê²© ì •ë³´
                "type": number             // ì¢Œì„ ìœ í˜•ë³„ ê°€ê²©
            },
            "age_limit": string | null,    // ê´€ëŒì—°ë ¹
            "booking_limit": string | null, // ì˜ˆë§¤ì œí•œ
            "selling_platform": "INTERPARK",  // íŒë§¤ì²˜
            "ticket_status": boolen,       // í‹°ì¼“ ì˜¤í”ˆ ì—¬ë¶€
            "ticket_open_dates": {       // í‹°ì¼“ ì˜¤í”ˆ ì¼ì •
                    "round": YYYY-MM-DDTHH:MM:SS
                }
            "booking_link": string | null,  // ì˜ˆë§¤ ë§í¬
            "additional_info": "ì´ê²ƒì€ í•˜ë‚˜ì˜ ë¬¸ìì—´ì…ë‹ˆë‹¤."  // ì¶”ê°€ì •ë³´
        }"""

    def parse_single_concert(self, concert_text: str) -> dict:
        """ë‹¨ì¼ ê³µì—° ì •ë³´ íŒŒì‹±"""
        prompt = f"""ë‹¤ìŒ ê³µì—° ì •ë³´ë¥¼ íŒŒì‹±í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.

        ê·œì¹™:
        1. ë‚ ì§œì™€ ì‹œê°„ì€ ëª¨ë‘ YYYY-MM-DDTHH:MM:SS í˜•ì‹ì„ ì‚¬ìš©
        2. ê°€ê²©ì€ ìˆ«ìë¡œ ë³€í™˜ (ì˜ˆ: "90,000ì›" â†’ 90000)
        3. ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° null ì‚¬ìš©
        4. ticket_statusëŠ” True ë˜ëŠ” False ì¤‘ í•˜ë‚˜ë¡œ í‘œì‹œ
        5. ticket_open_datesëŠ” ë‹¤ìŒ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ:
           - keyëŠ” ì˜ˆë§¤ ìœ í˜• ë˜ëŠ” íšŒì°¨ ë²ˆí˜¸
           - valueëŠ” YYYY-MM-DDTHH:MM:SS í˜•ì‹ì˜ ë‚ ì§œ
        6. ë‹¤ìŒ í•„ë“œë“¤ì€ ê°ê° ì •í•´ì§„ ê°’ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤:
            genreëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ:
            ["ë°œë¼ë“œ","ëŒ„ìŠ¤","ë©/í™í•©","ì•„ì´ëŒ","R&B/Soul","ì¸ë””ìŒì•…","ë¡/ë©”íƒˆ","ì„±ì¸ê°€ìš”/íŠ¸ë¡œíŠ¸",
            "í¬í¬/ë¸”ë£¨ìŠ¤","ì¼ë ‰íŠ¸ë¡œë‹ˆì¹´","í´ë˜ì‹","ì¬ì¦ˆ","J-POP","POP","í‚¤ì¦ˆ","CCM","êµ­ì•…"]
       
            concert_moodëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ:
            ["Emotional","Energetic","Dreamy","Grand","Calm","Fun","Intense"]
       
            concert_styleëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ:
            ["Live Band","Acoustic","Orchestra","Solo Performance","Dance Performance","Theatrical Concert"]
       
            concert_typeëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ:
            ["Festival","Concert","Music Show","Fan Meeting","Talk Concert"]
        
        ê³µì—° ì •ë³´:
        {concert_text}
        """

        messages = [
            {"role": "system", "content": self.system_message},
            {"role": "user", "content": prompt}
        ]

        try:
            response = self.client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=messages,
                response_format={"type": "json_object"},
                temperature=0.1  # ì¼ê´€ëœ ì¶œë ¥ì„ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©
            )

            # JSON íŒŒì‹± ê²€ì¦
            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            print(f"Error parsing concert data: {e}")
            return None

    def parse_multiple_concerts(self, crawled_concerts: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ ê³µì—° ì •ë³´ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
        results = []
        for idx, concert_text in enumerate(crawled_concerts):
            try:
                # API ë ˆì´íŠ¸ ë¦¬ë°‹ ê³ ë ¤
                if idx > 0:
                    time.sleep(0.5)

                print(f"Parsing concert {idx + 1}/{len(crawled_concerts)}")
                result = self.parse_single_concert(concert_text)

                if result:
                    results.append(result)
                    print(f"Successfully parsed concert {idx + 1}")
                else:
                    print(f"Failed to parse concert {idx + 1}")

            except Exception as e:
                print(f"Error processing concert {idx + 1}: {e}")
                continue

        return results

"""ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ í¬ë¡¤ë§ í•¨ìˆ˜"""
async def crawl_with_retry(max_retries: int = 1):
    min_num_of_error = 100
    final_crawled_concert = False
    parsed_results = False
    for attempt in range(1,max_retries+1):
        try:
            # ëœë¤ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€
            await asyncio.sleep(uniform(3, 5))

            # í¬ë¡¤ë§ ì‹œë„
            crawrled_data, num_of_error = await crawl_and_parse_concerts(page_num=attempt)
            print("ì—ëŸ¬ìˆ˜ :",num_of_error)
            if parsed_results:
                parsed_results.append(crawrled_data)
            else:
                parsed_results=crawrled_data
            # if num_of_error < min_num_of_error:
            #     min_num_of_error = num_of_error
            #     final_crawled_concert = crawrled_data

            #     if parsed_results:
            #         # GPT íŒŒì‹± ì²˜ë¦¬
            #         parser = ConcertParser()
            #         parsed_results.append(parser.parse_multiple_concerts(final_crawled_concert))
            #         print(parsed_results)
            #     else:
            #         # GPT íŒŒì‹± ì²˜ë¦¬
            #         parser = ConcertParser()
            #         parsed_results=parser.parse_multiple_concerts(final_crawled_concert)
            #         print(parsed_results)

            # else:
            #     print("í¬ë¡¤ë§ ë¬¸ì œ")

        except Exception as e:
            print(f"Attempt {attempt + 1} failed for crawling",e)

        await asyncio.sleep(5)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

    return parsed_results

    # if final_crawled_concert:
    #     # GPT íŒŒì‹± ì²˜ë¦¬
    #     parser = ConcertParser()
    #     parsed_results = parser.parse_multiple_concerts(final_crawled_concert)
    #     print(parsed_results)
    #     return parsed_results
    # else:
    #     print("í¬ë¡¤ë§ ë¬¸ì œ")


async def crawl_and_parse_concerts(page_num: int = 0):
    crawled_concerts = []
    errer_crawled_concerts = 0

    # ë©”ì¸ í˜ì´ì§€(ì¥ë¥´ : ì½˜ì„œíŠ¸) í¬ë¡¤ë§
    url = "https://tickets.interpark.com/contents/notice?Genre=CONCERT"
    # URL ì ‘ì†
    driver.get(url)
    # ì•”ì‹œì  ëŒ€ê¸°
    driver.implicitly_wait(5)

    SCROLL_AMOUNT = 700
    SCROLL_WAIT = 1.5
    MAX_SCROLL_END_RETRY = 3

    seen_labels = set()
    crawled_concerts = []

    scroll_end_counter = 0
    prev_seen_count = 0

    while True:
        items = driver.find_elements(By.CSS_SELECTOR, "a.TicketItem_ticketItem__")
        print(f"í˜„ì¬ í™”ë©´ ê³µì—° ìˆ˜: {len(items)}")

        new_labels = []
        for item in items:
            try:
                label = item.get_attribute("gtm-label")
                if label and label not in seen_labels:
                    new_labels.append(label)
                    seen_labels.add(label)
            except:
                continue

        for label in new_labels:
            try:                
                # í…ìŠ¤íŠ¸ ì •ë³´
                try:
                    texts = item.find_elements(By.CSS_SELECTOR, "ul.NoticeItem_contentsWrap__y1tdg li")
                    info_list = [t.text.strip() for t in texts]
                    info_str = ", ".join(info_list)
                    print(f"ê³µì—°ì •ë³´: {info_str}")
                except:
                    pass

                # ìƒì„¸ ë§í¬
                # ë§¤ë²ˆ freshí•˜ê²Œ í´ë¦­í•  ìš”ì†Œ ë‹¤ì‹œ ì°¾ê¸°
                clickable = driver.find_element(By.CSS_SELECTOR, f"a[gtm-label='{label}']")
                clickable.click()

                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "article.DetailSummary_infoBox__5we4P"))
                )

                current_url = driver.current_url
                print(f"ğŸŸï¸ [{label}] ìƒì„¸ ë§í¬:", current_url)

                concert_text = f"""
                    ê³µì—°ëª…: {label}
                    ê³µì—° ì •ë³´: {info_str}
                    ì˜ˆë§¤ë§í¬: {current_url}
                    """
                crawled_concerts.append(concert_text)
                print(concert_text)

                driver.back()
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "a.TicketItem_ticketItem__"))
                )
            except Exception as e:
                print(f"âš ï¸ a[gtm-label='{label}'] -- ì²˜ë¦¬ ì¤‘ ì—ëŸ¬:", e)
                continue

        # ìŠ¤í¬ë¡¤ ì¢…ë£Œ ê°ì§€
        if len(seen_labels) == prev_seen_count:
            scroll_end_counter += 1
            if scroll_end_counter >= MAX_SCROLL_END_RETRY:
                print("âœ… ë” ì´ìƒ ìƒˆë¡œìš´ í•­ëª© ì—†ìŒ. ì¢…ë£Œ.")
                break
        else:
            scroll_end_counter = 0
            prev_seen_count = len(seen_labels)

        driver.execute_script(f"window.scrollBy(0, {SCROLL_AMOUNT});")
        time.sleep(SCROLL_WAIT)


    return crawled_concerts, errer_crawled_concerts

    #     # iframe ë‚´ë¶€ URL í¬ë¡¤ë§
    #     result = await crawler.arun(
    #         url=url,
    #         headers=headers,
    #         css_selector="td.subject",  # í•„ìš”í•œ ë°ì´í„° ì„ íƒ
    #         # process_iframes=False,
    #         config=config
    #     )
    #     print("actural URL : ",result.url)
        
    #     # BeautifulSoupìœ¼ë¡œ ë°ì´í„° íŒŒì‹±
    #     iframe_soup = BeautifulSoup(result.html, "html.parser")
    #     rows = iframe_soup.select_one("div.InfiniteList_ticket-list__dfe68")
    #     # ê²°ê³¼ ì²˜ë¦¬
    #     for row in rows:
    #         print("row:",row)
    #         await asyncio.sleep(1)
    #         #  ê° ê³µì—° ì •ë³´ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    #         concert_info = {}

    #         # <td class="subject">ì™€ <a> íƒœê·¸ ì²˜ë¦¬
    #         subject_tag = row.select_one("td.subject a")
    #         # ì¥ë¥´ ê°€ì ¸ì˜¤ê¸°
    #         type_tag = row.select_one("td.type")

    #         # ì´ ë¶€ë¶„ì— ì›í•˜ëŠ” íƒ€ì…ì˜ ì¥ë¥´ë§Œ ê°€ì ¸ ì˜¬ ìˆ˜ ìˆë‹¤
    #         if type_tag and type_tag.text:
    #             concert_info['genre'] = type_tag.text.strip()
    #         else:
    #             continue

    #         if not concert_info.get('genre','') in ['ì½˜ì„œíŠ¸','HOT']:
    #             continue

    #         if subject_tag:
    #             concert_info['title'] = subject_tag.get_text(strip=True)
    #             link = subject_tag.get("href")
    #             absolute_link = urljoin(url, link)
    #             print("ë§í¬:",absolute_link)
    #             concert_info['link'] = absolute_link

    #             # new <img> íƒœê·¸ í™•ì¸ (ìƒˆë¡œ ëœ¬ ê³µì§€)
    #             new_img_tag = row.select_one("td.subject img.ico_new")
    #             concert_info['is_new'] = bool(new_img_tag)

    #             print("new img tag :",new_img_tag)

    #             if absolute_link:
    #                 await asyncio.sleep(3)  # ìƒì„¸ë§í¬ í¬ë¡¤ë§ ì „ ëŒ€ê¸°

    #                 async with AsyncWebCrawler(verbose=True) as crawler_concert:
    #                     # ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
    #                     result_concert = await crawler_concert.arun(url=absolute_link,config=config)

    #                     if not result_concert.success:
    #                         print(f"Crawl failed: {result_concert.error_message}")
    #                         print(f"Status code: {result_concert.status_code}")
    #                         errer_crawled_concerts += 1

    #                 # BeautifulSoup ê°ì²´ ìƒì„±
    #                 soup_concert = BeautifulSoup(result_concert.html, 'html.parser')

    #                 # í¬ìŠ¤í„° ì‚¬ì§„
    #                 poster_div = soup_concert.select_one("div.DetailSummary_imageContainer__OmWus")
    #                 if poster_div:
    #                     poster_tag = poster_div.find("img")  # <img> íƒœê·¸ ì°¾ê¸°
    #                     if poster_tag :
    #                         img_url = poster_tag.get("src")
    #                         # í”„ë¡œí† ì½œì´ ì—†ëŠ” ê²½ìš° http: ì¶”ê°€
    #                         if img_url.startswith("//"):
    #                             img_url = "https:" + img_url
    #                         concert_info['poster'] = img_url
    #                 else:
    #                     errer_crawled_concerts += 1
    #                     if poster_div and poster_div.text:
    #                         concert_info['info'] = poster_div.text.strip()

    #                 # ê³µì—° ìš”ì•½ ì •ë³´
    #                 concert_summary_div = soup_concert.select_one("article.DetailSummary_infoBox__5we4P")
    #                 if concert_summary_div and concert_summary_div.text:
    #                     concert_info['summary'] = concert_summary_div.text.strip()
    #                 else:
    #                     print("concert_summary_div ì—†ìŒ ë˜ëŠ” ë‚´ìš© ì—†ìŒ:", concert_summary_div)
    #                     concert_info['summary'] = None
                    
    #                 # ê³µì—° ë””í…Œì¼ ì •ë³´
    #                 concert_detail_div = soup_concert.select_one(".DetailInfo_contents__grsx5.DetailInfo_isOld__4UynI")
    #                 if concert_detail_div and concert_detail_div.text:
    #                     concert_info['description'] = concert_detail_div.text.strip()
    #                 else:
    #                     print("concert_detail_div ì—†ìŒ ë˜ëŠ” ë‚´ìš© ì—†ìŒ:", concert_detail_div)
    #                     concert_info['description'] = None
                        
    #                 # <a class="btn_book"> íƒœê·¸ ì²˜ë¦¬ ì˜ˆì•½ ë²„íŠ¼
    #                 book_button_tag = soup_concert.select_one("button.DetailBooking_bookingBtn__uvSid")
    #                 if book_button_tag:
    #                     concert_info['booking_link'] = absolute_link
    #                     concert_info['ticket_status'] = "True"
    #                 else :
    #                     concert_info['ticket_status'] = "False"

    #                 # í¬ë¡¤ë§ëœ ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜
    #                 concert_text = f"""
    #                 ê³µì—°ëª…: {concert_info.get('title', '')}
    #                 ê³µì—° í¬ìŠ¤í„°: {concert_info.get('poster', '')}
    #                 ì¥ë¥´: {concert_info.get('genre', '')}
    #                 í‹°ì¼“ìƒíƒœ: {concert_info.get('ticket_status', '')}
    #                 ê³µì—°ì •ë³´: {concert_info.get('info', '')}
    #                 ê³µì—°ìš”ì•½: {concert_info.get('summary', '')}
    #                 ê³µì—°ì„¤ëª…: {concert_info.get('description', '')}
    #                 ì˜ˆë§¤ë§í¬: {concert_info.get('booking_link', 'í‹°ì¼“ ë¯¸ì˜¤í”ˆ')}
    #                 """
    #                 print(concert_text)
    #                 crawled_concerts.append(concert_text)

    # return crawled_concerts, errer_crawled_concerts

# ì‹¤í–‰ ì½”ë“œ
async def main():
    # í¬ë¡¤ë§ ì½”ë“œ ì‹¤í–‰
    results = await crawl_with_retry()

    # ê²°ê³¼ ì œì¶œ
    current_dir = os.getcwd()
    target_dir = os.path.join(current_dir, 'crawl_new_concerts')
    os.makedirs(target_dir, exist_ok=True)
    # í˜„ì¬ ë‚ ì§œë¥¼ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì €ì¥
    current_date = datetime.now().strftime('%Y-%m-%d')  # 'YYYY-MM-DD' í˜•ì‹
    file_path = os.path.join(target_dir, f'{current_date}.json')

    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # HTTP í—¤ë” ì„¤ì • (JSON í˜•ì‹)
    headers = {"Content-Type": "application/json"}

    # # JSON íŒŒì¼ ì—´ê¸° ë° ë°ì´í„° ë¡œë“œ
    # with open(f'crawl_new_concerts/{file_path}', 'r', encoding='utf-8') as f:
    #     data_list = json.load(f)  # ì´ì œ data_listëŠ” Python ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤

    # response = requests.post(API_URL, headers=headers, json=data_list)  # `json=data` ì‚¬ìš©

    # ì‘ë‹µ í™•ì¸
    # print("ì‘ë‹µ ì½”ë“œ:", response.status_code)

if __name__ == "__main__":
    asyncio.run(main())

