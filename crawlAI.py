import asyncio
import os
import time
import json
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from parseDetail import parseDetail
from makeJson import makeJson, ConcertParser


OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
API_URL = "https://dev.bolmal.shop/save"

# ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì„¤ì •
options = Options()
options.page_load_strategy = 'normal'
options.add_experimental_option("detach", True)
driver = webdriver.Chrome(options=options)

async def crawlConcerts() -> list[str]:
    crawledConcerts = []
    errerCrawledConcerts = 0

    # ë©”ì¸ í˜ì´ì§€(ì¥ë¥´ : ì½˜ì„œíŠ¸) í¬ë¡¤ë§
    url = "https://tickets.interpark.com/contents/notice?Genre=CONCERT"
    # URL ì ‘ì†
    driver.get(url)
    # ì•”ì‹œì  ëŒ€ê¸°
    driver.implicitly_wait(5)

    SCROLL_AMOUNT = 700
    SCROLL_WAIT = 1.5
    MAX_SCROLL_END_RETRY = 3

    seenLabels = set()
    crawledConcerts = []

    scrollEndCounter = 0
    prevSeenCount = 0

    while True:
        items = driver.find_elements(By.CSS_SELECTOR, "a.TicketItem_ticketItem__")
        print(f"í˜„ì¬ í™”ë©´ ê³µì—° ìˆ˜: {len(items)}")

        newLabels = []
        for item in items:
            try:
                label = item.get_attribute("gtm-label")
                if label and label not in seenLabels:
                    newLabels.append(label)
                    seenLabels.add(label)
            except:
                continue

        for label in newLabels:
            try:                
                # í…ìŠ¤íŠ¸ ì •ë³´
                try:
                    texts = item.find_elements(By.CSS_SELECTOR, "ul.NoticeItem_contentsWrap__y1tdg li")
                    infoList = [t.text.strip() for t in texts]
                    infoStr = ", ".join(infoList)
                    print(f"ê³µì—°ì •ë³´: {infoStr}")
                except:
                    pass

                # ìƒì„¸ ë§í¬
                # ë§¤ë²ˆ freshí•˜ê²Œ í´ë¦­í•  ìš”ì†Œ ë‹¤ì‹œ ì°¾ê¸°
                clickable = driver.find_element(By.CSS_SELECTOR, f"a[gtm-label='{label}']")
                clickable.click()

                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "article.DetailSummary_infoBox__5we4P"))
                )

                currentUrl = driver.current_url
                print(f"ğŸŸï¸ [{label}] ìƒì„¸ ë§í¬:", currentUrl)

                concertText = f"""
                    ê³µì—°ëª…: {label}
                    ê³µì—° ì •ë³´: {infoStr}
                    ì˜ˆë§¤ë§í¬: {currentUrl}
                    """
                crawledConcerts.append(concertText)
                print(concertText)

                driver.back()
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "a.TicketItem_ticketItem__"))
                )
            except Exception as e:
                print(f"âš ï¸ a[gtm-label='{label}'] -- ì²˜ë¦¬ ì¤‘ ì—ëŸ¬:", e)
                continue

        # ìŠ¤í¬ë¡¤ ì¢…ë£Œ ê°ì§€
        if len(seenLabels) == prevSeenCount:
            scrollEndCounter += 1
            if scrollEndCounter >= MAX_SCROLL_END_RETRY:
                print("âœ… ë” ì´ìƒ ìƒˆë¡œìš´ í•­ëª© ì—†ìŒ. ì¢…ë£Œ.")
                break
        else:
            scrollEndCounter = 0
            prevSeenCount = len(seenLabels)

        driver.execute_script(f"window.scrollBy(0, {SCROLL_AMOUNT});")
        time.sleep(SCROLL_WAIT)


    return crawledConcerts, errerCrawledConcerts

async def crawlConcert() -> list[str]:
    # ì˜¤í”ˆì˜ˆì • ê³µì—° í¬ë¡¤ë§
    crawrledData, numOfError = await crawlConcerts()
    print("ì—ëŸ¬ìˆ˜ :",numOfError)

    # ê³µì—° ìƒì„¸ í¬ë¡¤ë§
    finalOutput = parseDetail(crawrledData)

    # json í˜•ì‹ìœ¼ë¡œ ì €ì¥
    # result = makeJson(finalOutput,api_key=OPENAI_API_KEY)
    result = ConcertParser.make_json_with_langchain(finalOutput,OPENAI_API_KEY)
    return result

# ì‹¤í–‰ ì½”ë“œ
async def main():
    # í¬ë¡¤ë§ ì½”ë“œ ì‹¤í–‰
    results = await crawlConcert()

    # ê²°ê³¼ ì œì¶œ
    current_dir = os.getcwd()
    target_dir = os.path.join(current_dir, 'crawl_new_concerts')
    os.makedirs(target_dir, exist_ok=True)

    # í˜„ì¬ ë‚ ì§œë¥¼ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì €ì¥
    current_date = datetime.now().strftime('%Y-%m-%d')  # 'YYYY-MM-DD' í˜•ì‹
    file_path = os.path.join(target_dir, f'{current_date}.json')

    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # HTTP í—¤ë” ì„¤ì • (JSON í˜•ì‹)
    headers = {"Content-Type": "application/json"}

    # # JSON íŒŒì¼ ì—´ê¸° ë° ë°ì´í„° ë¡œë“œ
    # with open(f'crawl_new_concerts/{file_path}', 'r', encoding='utf-8') as f:
    #     data_list = json.load(f)  # ì´ì œ data_listëŠ” Python ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤

    # response = requests.post(API_URL, headers=headers, json=data_list)  # `json=data` ì‚¬ìš©

    # ì‘ë‹µ í™•ì¸
    # print("ì‘ë‹µ ì½”ë“œ:", response.status_code)

if __name__ == "__main__":
    asyncio.run(main())

